{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-02T22:19:09.760204Z",
     "start_time": "2024-12-02T22:19:09.755924Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.ops.numpy_ops.np_dtypes import int64\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays\n"
   ],
   "outputs": [],
   "execution_count": 280
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## generating data sets\n",
    "this code will take 2 params, the number of samples and the number of features and the code will generate the data sets randomly"
   ],
   "id": "972fe9b2626a2c5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T22:19:09.776533Z",
     "start_time": "2024-12-02T22:19:09.770212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_dataset(num_samples = 100, n_features = 10):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset with a specified number of features and samples.\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): Number of data points to generate.\n",
    "        n_features (int): Number of features in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        X_train (ndarray): Feature matrix of shape (num_samples, n_features).\n",
    "        y_train (ndarray): Target array of shape (num_samples,).\n",
    "    \"\"\"\n",
    "    # Initialize the feature matrix\n",
    "    X_train = np.zeros((num_samples, n_features),dtype=int64)\n",
    "\n",
    "    # Populate features dynamically\n",
    "    for i in range(n_features):\n",
    "        if i == 0:  # Feature 1: Area in square feet\n",
    "            X_train[:, i] = np.random.randint(800, 4000, num_samples)\n",
    "        elif i == 1:  # Feature 2: Bedrooms (correlated with area)\n",
    "            X_train[:, i] = np.clip((X_train[:, 0] // 1000) + np.random.randint(-1, 2, num_samples), 1, 5)\n",
    "        elif i == 2:  # Feature 3: Bathrooms (correlated with bedrooms)\n",
    "            X_train[:, i] = np.clip(X_train[:, 1] + np.random.randint(-1, 2, num_samples), 1, 3)\n",
    "        elif i == 3:  # Feature 4: Age of property\n",
    "            X_train[:, i] = np.random.randint(5, 50, num_samples)\n",
    "        else:  # Additional features: Random values within a defined range\n",
    "            X_train[:, i] = np.random.randint(1, 100, num_samples)\n",
    "\n",
    "    # Generate labels with a relationship to the features\n",
    "    # Using coefficients for up to 4 features, extendable for additional features\n",
    "    coefficients = np.array([50, 30, 20, -10] + [5] * (n_features - 4))  # Default coefficients\n",
    "    noise = np.random.normal(0, 500, num_samples)  # Small noise for realism\n",
    "    y_train = (X_train @ coefficients[:n_features] + noise).astype(int)\n",
    "\n",
    "    return X_train, y_train"
   ],
   "id": "184bb744c65afe3e",
   "outputs": [],
   "execution_count": 281
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"toc_15456_3\"></a>\n",
    "## Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ],
   "id": "3578424bac82650e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T22:19:09.790846Z",
     "start_time": "2024-12-02T22:19:09.786316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(W,b,X_record):\n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    return np.dot(W,X_record) + b"
   ],
   "id": "8b697ebd0d091106",
   "outputs": [],
   "execution_count": 282
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"toc_15456_4\"></a>\n",
    "# Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ],
   "id": "6458822f1a81e3ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T22:19:09.805128Z",
     "start_time": "2024-12-02T22:19:09.800010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cost(W,b,X,y):\n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    #iterative code\n",
    "    m = X.shape[0]\n",
    "    # c = 0.0\n",
    "    # for i in range(m):\n",
    "    #     f_wb_i = np.dot(W,X[i]) + b\n",
    "    #     c += (f_wb_i - y[i])**2\n",
    "    # c= c/2*m\n",
    "    \n",
    "    #numpy code\n",
    "    \n",
    "    #it will iterate over the rows of X and dot it by W and add the b and then it will be stored in f_wb[i]and that's for every row of X  \n",
    "    F_WB = np.dot(X,W) + b\n",
    "    Squared_Error = (F_WB  - y) ** 2\n",
    "    c = np.sum(Squared_Error)/(2*m)\n",
    "    \n",
    "    return c"
   ],
   "id": "52a47113b5a674c2",
   "outputs": [],
   "execution_count": 283
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "## 5 Gradient Descent With Multiple Variables\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
   ],
   "id": "d12e69b33ed4b66d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dot Product Explanation\n",
    "\n",
    "The dot product will return an array, where each element contains the dot product of a record (row) with the weights vector.\n",
    "\n",
    "#### Example\n",
    "Consider the following table with 4 features and \\(m\\) records:\n",
    "\n",
    "| Feature 1 | Feature 2 | Feature 3 | Feature 4 | Target (y) |\n",
    "|-----------|-----------|-----------|-----------|------------|\n",
    "| 10        | 20        | 30        | 40        | 203        |\n",
    "| ..        | ..        | ..        | ..        | ...        |\n",
    "\n",
    "and the weights of features\n",
    "\n",
    "| W of Feature 1 | W of Feature 2 | W of Feature 3 | W of Feature 4 |\n",
    "|----------------|----------------|----------------|----------------|\n",
    "| 1              | 0.1            | 2              | -22            | \n",
    "\n",
    "#### Dot Product Computation:\n",
    "the dot product will return an array with m elements [x[i] . w[i], ... ] \n",
    "or [ [10,20,30,40] . [1,0.1,2,-22] , ... ]"
   ],
   "id": "b2cd59909386c83d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T22:19:09.819153Z",
     "start_time": "2024-12-02T22:19:09.813375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def partial_derivative(W,b,X,y):\n",
    "    \"\"\"\n",
    "    Compute the partial derivatives of the cost function with respect to W and b.\n",
    "\n",
    "    Args:\n",
    "      W (ndarray (n,)): Model parameters (weights)\n",
    "      b (scalar): Model parameter (bias)\n",
    "      X (ndarray (m, n)): Input data (m records, n features)\n",
    "      y (ndarray (m,)): Target values\n",
    "\n",
    "    Returns:\n",
    "      DJ_DW (ndarray (n,)): Partial derivatives with respect to W\n",
    "      dj_db (scalar): Partial derivative with respect to b\n",
    "    \"\"\"\n",
    "    number_of_features = n = X.shape[1]\n",
    "    number_of_records = m = X.shape[0]\n",
    "    DJ_DW = np.zeros(number_of_features)\n",
    "    dj_db = 0.0\n",
    "    errors = np.dot(X,W) + b - y         #the error for every record\n",
    "    dj_db = np.sum(errors)/m  \n",
    "    for j in range(number_of_features):\n",
    "        DJ_DW[j] = (np.sum(errors * X[:,j]))/m\n",
    "    return DJ_DW, dj_db\n"
   ],
   "id": "ffde8f33ca6fa47a",
   "outputs": [],
   "execution_count": 284
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### now into gradient descent code",
   "id": "9714df9ab5fb5a34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T22:19:09.834509Z",
     "start_time": "2024-12-02T22:19:09.828571Z"
    }
   },
   "cell_type": "code",
   "source": [
    " def gradient_descent(Inint_W,init_b,alpha,times,X_Training,y_training): \n",
    "     \n",
    "    W = Inint_W\n",
    "    number_of_features = n = W.shape[0]\n",
    "    b = init_b\n",
    "        \n",
    "    for i in range(times):\n",
    "        flag = True\n",
    "        DJ_DW,dj_db = partial_derivative(W,b,X_Training,y_training)\n",
    "        flag &= (abs(dj_db * alpha)<1e-7)\n",
    "        b = b - alpha*dj_db\n",
    "        flag &= ((np.sum(abs(DJ_DW * alpha))/number_of_features) < 1e-7)\n",
    "        W = W - DJ_DW * alpha\n",
    "        if(flag):\n",
    "            print(i); break\n",
    "    return W,b\n",
    " \n",
    " def gradient_descent_algorithm(X_Training,y_training):\n",
    "     number_of_features = X_Training.shape[1]\n",
    "     Init_W = np.full(number_of_features,0)\n",
    "     init_b = 0.\n",
    "     alpha = 5.0e-8\n",
    "     times = 10000\n",
    "     return gradient_descent(Init_W,init_b,alpha,times,X_Training,y_training)\n",
    "        "
   ],
   "id": "7f5a81b8d0364ba1",
   "outputs": [],
   "execution_count": 285
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Running the Code and taking Predication",
   "id": "f05466cfa282f2cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T22:19:11.570010Z",
     "start_time": "2024-12-02T22:19:09.845915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_samples = 10000 ; num_features = 4\n",
    "\n",
    "x_train, y_train = generate_dataset(num_samples,num_features)\n",
    "# x_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "# y_train = np.array([460, 232, 178])\n",
    "\n",
    "W,b = gradient_descent_algorithm(x_train,y_train)\n",
    "\n",
    "print(\"the Weights are: \",W)\n",
    "print(f\"the bias is: {b:0.2f}\")\n",
    "print(predict(W,b,np.array([1000,2,3,4])))\n",
    "\n"
   ],
   "id": "880bddc8ad3defcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Weights are:  [ 5.00e+01  4.91e-02  3.19e-02 -7.79e-01]\n",
      "the bias is: 0.00\n",
      "49949.26179563136\n"
     ]
    }
   ],
   "execution_count": 286
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
